"""Multi-head Attention Package.

This package contains the implementation of Multi-head Attention mechanisms,
which are core components of the Transformer model architecture.
Multi-head Attention improves the ability of the model
to capture different aspects of the input sequence's dependencies
by applying multiple attention heads in parallel.
This enhances the expressiveness and generalization capabilities
of the Transformer model.
"""
